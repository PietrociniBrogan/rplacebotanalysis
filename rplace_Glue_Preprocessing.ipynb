{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"toc-autonumbering": true
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.2X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.2X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.2X\nNumber of Workers: 5\nSession ID: 7421d826-4dbd-4da9-9123-1532c7381994\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\nWaiting for session 7421d826-4dbd-4da9-9123-1532c7381994 to get into ready status...\nSession 7421d826-4dbd-4da9-9123-1532c7381994 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Example: Read Parquet from S3\n\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\ndyf = glueContext.create_dynamic_frame.from_options(\n    connection_type = \"s3\", \n    connection_options = {\"paths\": [\"s3://noturs/merged_file.parquet\"]}, \n    format = \"parquet\"\n)\n#dyf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\n#df.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Prepare Data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, TimestampType\nfrom pyspark.sql.types import StructType, StructField, StringType\n\n# Split the 'coordinate' column into two separate columns 'x' and 'y'\ndf2 = df.withColumn('x', F.split(df['coordinate'], ',').getItem(0).cast('int')) \\\n       .withColumn('y', F.split(df['coordinate'], ',').getItem(1).cast('int'))\n\n#Convert timestamp to timstamp data type\ndf2 = df2.withColumn(\"timestamp\", df2[\"timestamp\"].cast(TimestampType()))\n\n# Show the updated DataFrame to verify the new columns\n#df2.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Bot Detection Code",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.window import Window\nimport pyspark.sql.functions as F\n\n# Window spec to order by user and timestamp\nwindowSpec = Window.partitionBy(\"user\").orderBy(\"timestamp\")\n\n# Add a column with the previous timestamp for each user\ndf2 = df2.withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(windowSpec))\n\n# Calculate the time difference in seconds between the current and previous actions\ndf2 = df2.withColumn(\"time_diff\", (F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\")))\n\n# Identify actions that are exactly or nearly 5 minutes apart (300 seconds)\n# Adjust the tolerance as needed to define \"exactness\"\ntolerance = 2  # seconds\ndf2 = df2.withColumn(\"is_exact_timing\", F.abs(F.col(\"time_diff\") - 300) <= tolerance)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Add columns with the previous x and y coordinates for each user\ndf2 = df2.withColumn(\"prev_x\", F.lag(\"x\").over(windowSpec))\ndf2 = df2.withColumn(\"prev_y\", F.lag(\"y\").over(windowSpec))\n\n# Calculate the difference in coordinates between the current and previous actions\ndf2 = df2.withColumn(\"coord_diff\", F.sqrt(F.pow(F.col(\"x\") - F.col(\"prev_x\"), 2) + F.pow(F.col(\"y\") - F.col(\"prev_y\"), 2)))\n\n# Flag actions that occur at the same or nearly the same coordinates\n# Adjust the threshold as needed based on your definition of \"nearby\"\ncoord_threshold = 3  # units\ndf2 = df2.withColumn(\"is_same_coord\", F.col(\"coord_diff\") <= coord_threshold)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Filter to potential bot actions based on timing or coordinate precision\npotential_bots = df2.filter(F.col(\"is_exact_timing\") & F.col(\"is_same_coord\"))\n\n# Group by user to count potential bot actions\nbot_counts = potential_bots.groupBy(\"user\").agg(F.count(\"*\").alias(\"num_bot_actions\"))\n\n# Optionally, filter users based on a threshold of bot-like actions\n# This threshold can be adjusted based on your analysis\nthreshold = 5  # Example threshold\nsuspected_bots = bot_counts.filter(F.col(\"num_bot_actions\") > threshold)\n\n#suspected_bots.show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Join the suspected bots DataFrame with the original DataFrame to filter actions by bots\nbot_actions = df2.join(suspected_bots, \"user\").select(df2[\"user\"], \"pixel_color\", \"x\", \"y\")\n\n# Select only the necessary columns before converting to Pandas DataFrame\nbot_actions_selected = bot_actions.select(\"pixel_color\", \"x\", \"y\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Convert back to dynamic frame and then Use glue to write parquet\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\n\nfrom awsglue.dynamicframe import DynamicFrame\n\n# Convert the coalesced Spark DataFrame to a Glue DynamicFrame\ndynamic_frame = DynamicFrame.fromDF(bot_actions_selected, glueContext, \"dynamic_frame\")\n\n# Write the DynamicFrame to S3\nglueContext.write_dynamic_frame.from_options(\n    frame = dynamic_frame,\n    connection_type = \"s3\",\n    connection_options = {\"path\": \"s3://bpkbbucket/last_minute/\"},\n    format = \"parquet\"  # Or your desired format\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f16fd340580>\n",
					"output_type": "stream"
				}
			]
		}
	]
}